"""Script d'entra√Ænement du mod√®le de classification de sentiments pour les commentaires de films.

Ce script :
1. Charge un dataset de commentaires de films avec leurs sentiments (positif/neutre/n√©gatif)
2. Transforme les textes en vecteurs num√©riques avec Word2Vec
3. Entra√Æne un r√©seau de neurones (CNN + LSTM) pour pr√©dire le sentiment
4. Sauvegarde les mod√®les entra√Æn√©s pour une utilisation ult√©rieure
"""

import os

# Configuration pour r√©duire les messages d'information de TensorFlow
# IMPORTANT : Ces lignes DOIVENT √™tre avant l'import de TensorFlow
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # 3 = Afficher seulement les erreurs (pas les warnings/infos)
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # D√©sactive les optimisations oneDNN (r√©duit les messages)

# Imports des biblioth√®ques n√©cessaires
import numpy as np  # Pour les calculs num√©riques et les tableaux
import pandas as pd  # Pour manipuler les donn√©es sous forme de tableaux
import matplotlib.pyplot as plt  # Pour cr√©er des graphiques (pas utilis√© actuellement)
import tensorflow as tf  # Framework de deep learning pour cr√©er le r√©seau de neurones

# Imports des modules sp√©cifiques
from dotenv import load_dotenv  # Pour charger les variables d'environnement depuis le fichier .env
from gensim.models import Word2Vec  # Pour cr√©er des embeddings de mots (Word2Vec)
from keras import layers  # Pour construire les couches du r√©seau de neurones
from spacy.tokenizer import Tokenizer  # Pour d√©couper le texte (pas utilis√© actuellement)
from spacy.lang.fr import French  # Pour le traitement du texte en fran√ßais
from sklearn.model_selection import train_test_split  # Pour diviser les donn√©es en train/test
from keras.callbacks import TensorBoard  # Pour visualiser l'entra√Ænement avec TensorBoard

def tokenize_corpus(comments):
    """Transforme une liste de commentaires en liste de mots (tokens).
    
    Cette fonction pr√©pare le texte pour Word2Vec :
    - Met tout en minuscules pour uniformiser
    - D√©coupe chaque commentaire en mots individuels
    - Enl√®ve la ponctuation (virgules, points, etc.)
    
    Args:
        comments: Liste de commentaires (textes bruts)
    
    Returns:
        Liste de listes de mots. Chaque sous-liste = les mots d'un commentaire
        Exemple: [["super", "film"], ["tr√®s", "mauvais", "sc√©nario"]]
    """
    tokens = []  # Liste qui contiendra tous les commentaires tokenis√©s
    nlp = French()  # Cr√©er un objet pour analyser le fran√ßais
    
    for comment in comments:
        comment = comment.lower()  # Convertir en minuscules (Film ‚Üí film)
        # nlp(comment) d√©coupe le texte et analyse chaque mot
        # x.text r√©cup√®re le mot, x.is_punct v√©rifie si c'est de la ponctuation
        tokens.append([x.text for x in nlp(comment) if not x.is_punct])
    return tokens

def fit_word2vec(tokens):
    """Entra√Æne un mod√®le Word2Vec pour transformer les mots en vecteurs num√©riques.
    
    Word2Vec apprend √† repr√©senter chaque mot comme un vecteur de nombres.
    Les mots similaires auront des vecteurs proches (ex: "excellent" et "super").
    
    Args:
        tokens: Liste de listes de mots provenant de tokenize_corpus
    
    Returns:
        Mod√®le Word2Vec entra√Æn√©
    """
    # Cr√©ation et entra√Ænement du mod√®le Word2Vec
    w2v = Word2Vec(
        sentences=tokens,  # Les commentaires tokenis√©s
        vector_size=W2V_SIZE,  # Taille des vecteurs (100 dimensions par d√©faut)
        min_count=W2V_MIN_COUNT,  # Ignore les mots qui apparaissent moins de 3 fois
        window=5,  # Contexte : regarde 5 mots avant et apr√®s pour apprendre
        workers=2  # Utilise 2 threads pour acc√©l√©rer l'entra√Ænement
    )
    
    # Sauvegarder le mod√®le pour pouvoir le r√©utiliser plus tard
    w2v.wv.save("models/w2v.wv")
    
    # Afficher des statistiques sur le vocabulaire appris
    print(f"\nüìä Informations sur le mod√®le Word2Vec:")
    print(f"- Taille du vocabulaire: {len(w2v.wv)} mots uniques")
    
    return w2v

def comment2vec(tokens, w2v):
    """Convertit chaque commentaire en une matrice de vecteurs Word2Vec.
    
    Transforme les mots en nombres pour que le r√©seau de neurones puisse les comprendre.
    Chaque commentaire devient une matrice de taille fixe (100 x 64).
    
    Args:
        tokens: Liste de listes de mots
        w2v: Mod√®le Word2Vec entra√Æn√©
    
    Returns:
        Array numpy 3D de forme (nb_commentaires, 100, 64)
        - Dimension 1: Chaque commentaire
        - Dimension 2: Les 100 dimensions du vecteur Word2Vec
        - Dimension 3: Les 64 mots maximum par commentaire
    """
    X = []  # Liste qui contiendra toutes les matrices
    
    # Pour chaque commentaire tokenis√©
    for i, row_tokens in enumerate(tokens):
        # Cr√©er une matrice vide (100 lignes x 64 colonnes) remplie de z√©ros
        # 100 lignes = dimensions Word2Vec, 64 colonnes = mots maximum
        row = np.zeros((W2V_SIZE, MAX_LENGTH))
        
        # Remplir la matrice avec les vecteurs des mots (max 64 mots)
        for j in range(min(MAX_LENGTH, len(row_tokens))):
            try:
                # Mettre le vecteur du mot j dans la colonne j
                row[:, j] = w2v.wv[row_tokens[j]]
            except KeyError:
                # Si le mot n'est pas dans le vocabulaire, on laisse des z√©ros
                continue
        
        X.append(row)
        
        # Afficher la progression tous les 1000 commentaires
        if i % 1000 == 0:
            print(f"{(i * 100) / len(tokens):.1f}% effectu√©.")
    
    # Convertir la liste en un seul tableau numpy 3D
    X = np.asarray(X)
    return X

def merge_feelings(data):
    """Pr√©pare les √©tiquettes (labels) pour l'entra√Ænement.
    
    Transforme les scores de sentiment en cat√©gories one-hot encoded.
    Le dataset contient 3 colonnes de scores (negative, neutral, positive).
    On garde seulement le sentiment dominant pour chaque commentaire.
    
    Args:
        data: DataFrame avec colonnes 'negative', 'neutral', 'positive'
    
    Returns:
        Array one-hot encoded (3 colonnes : une seule vaut 1, les autres 0)
        Exemple: [0, 0, 1] = positif, [1, 0, 0] = n√©gatif, [0, 1, 0] = neutre
    """
    # Extraire les 3 colonnes de sentiments
    y = data[["negative", "neutral", "positive"]]
    
    # argmax trouve l'indice de la valeur maximum (0=neg, 1=neutre, 2=pos)
    # get_dummies transforme en one-hot encoding
    y = pd.get_dummies(np.argmax(y.values, axis=1))
    return y

def create_rnn():
    """Cr√©e l'architecture du r√©seau de neurones pour la classification.
    
    Architecture hybride CNN + LSTM :
    - CNN (Convolution) : D√©tecte des patterns locaux dans le texte
    - LSTM : Comprend les s√©quences et le contexte
    - Dense : Couches finales pour la classification
    
    Returns:
        Mod√®le Keras non compil√©
    """
    return tf.keras.Sequential([
        # Couche d'entr√©e : attend des matrices de taille (100, 64)
        layers.Input(shape=(W2V_SIZE, MAX_LENGTH)),
        
        # Convolution 1D : d√©tecte des patterns de 3 mots cons√©cutifs
        # 32 filtres diff√©rents apprennent 32 patterns diff√©rents
        layers.Convolution1D(32, kernel_size=3, padding='same', activation='relu'),
        
        # MaxPooling : r√©duit la taille de moiti√© en gardant les valeurs importantes
        layers.MaxPool1D(2),
        
        # LSTM : r√©seau r√©current qui comprend les s√©quences
        # 128 neurones, return_sequences=True pour garder toute la s√©quence
        layers.LSTM(128, activation="tanh", return_sequences=True),
        
        # Dropout : d√©sactive al√©atoirement 10% des neurones (√©vite le surapprentissage)
        layers.Dropout(0.1),
        
        # Flatten : transforme la matrice en vecteur 1D pour les couches Dense
        layers.Flatten(),
        
        # Dense : couche classique avec 64 neurones
        layers.Dense(64, activation="tanh"),
        
        # Couche de sortie : 3 neurones (n√©gatif, neutre, positif)
        # Softmax garantit que la somme des probabilit√©s = 1
        layers.Dense(3, activation="softmax")
    ])
            
def main():
    """Fonction principale qui orchestre tout l'entra√Ænement."""
    
    # √âTAPE 1 : Chargement des donn√©es
    # Lit le fichier CSV et supprime les lignes avec des valeurs manquantes
    data = pd.read_csv("data/text_sentiment.csv").dropna()
    
    # Prend un √©chantillon al√©atoire (toujours le m√™me avec random_state=42)
    data = data.sample(NB_COMMENT, random_state=42)
    
    # √âTAPE 2 : Pr√©paration du texte
    # Transforme les commentaires en listes de mots
    tokens = tokenize_corpus(data["comment"])
    
    # √âTAPE 3 : Cr√©ation des embeddings Word2Vec
    # Apprend √† repr√©senter chaque mot comme un vecteur
    w2v = fit_word2vec(tokens)
    
    # √âTAPE 4 : Vectorisation des commentaires
    # Transforme chaque commentaire en matrice de vecteurs
    X = comment2vec(tokens, w2v)
    
    # √âTAPE 5 : Pr√©paration des labels
    # Transforme les sentiments en format one-hot
    y = merge_feelings(data)
    
    # √âTAPE 6 : Division train/test
    # 75% pour l'entra√Ænement, 25% pour le test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    
    # √âTAPE 7 : Cr√©ation et configuration du mod√®le
    rnn = create_rnn()
    
    # Compilation : d√©finit comment le mod√®le va apprendre
    rnn.compile(
        optimizer='adam',  # Algorithme d'optimisation (ajuste les poids)
        loss="categorical_crossentropy",  # Fonction de perte pour classification multi-classes
        metrics=['categorical_accuracy']  # M√©trique √† surveiller (% de bonnes pr√©dictions)
    )
    
    # Affiche un r√©sum√© de l'architecture du mod√®le
    rnn.summary()
    
    # √âTAPE 8 : Entra√Ænement
    # TensorBoard permet de visualiser l'entra√Ænement en temps r√©el
    tensorboard_callback = TensorBoard("logs/rnn_movies_comments")
    
    # Entra√Ænement du mod√®le
    rnn.fit(
        x=X_train, y=y_train,  # Donn√©es d'entra√Ænement
        validation_data=(X_test, y_test),  # Donn√©es de validation
        epochs=30,  # Nombre de passes sur les donn√©es
        batch_size=32,  # Traite 32 commentaires √† la fois
        callbacks=[tensorboard_callback]  # Pour le monitoring
    )
    
    # √âTAPE 9 : Sauvegarde
    # Sauvegarde le mod√®le entra√Æn√© pour utilisation future
    rnn.save("models/comment_sentiment_rnn.keras")
    print("\n‚úÖ Mod√®le sauvegard√© dans models/comment_sentiment_rnn.keras")

# Point d'entr√©e du script
if __name__ == "__main__":
    # Charge les variables d'environnement depuis le fichier .env
    load_dotenv()
    
    # Configuration des hyperparam√®tres (avec valeurs par d√©faut si non d√©finies)
    W2V_SIZE = int(os.getenv("W2V_SIZE", 100))  # Dimension des vecteurs Word2Vec
    W2V_MIN_COUNT = int(os.getenv("W2V_MIN_COUNT", 3))  # Fr√©quence minimale des mots
    MAX_LENGTH = int(os.getenv("MAX_LENGTH", 64))  # Nombre max de mots par commentaire
    NB_COMMENT = int(os.getenv("NB_COMMENT", 10000))  # Nombre de commentaires √† utiliser
    
    # Lance le processus d'entra√Ænement
    main()