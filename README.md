# üé¨ Analyse de Sentiments - Commentaires de Films

Un syst√®me de deep learning pour analyser automatiquement le sentiment (positif, neutre, n√©gatif) des commentaires de films en fran√ßais.

## üìã Description

Ce projet utilise :
- **Word2Vec** pour transformer les mots en vecteurs num√©riques
- **CNN + LSTM** pour comprendre le contexte et classifier les sentiments
- **TensorFlow/Keras** pour le deep learning
- **Spacy** pour le traitement du texte en fran√ßais

## üèóÔ∏è Architecture

```
Commentaire texte
    ‚Üì
Tokenisation (Spacy)
    ‚Üì
Vectorisation (Word2Vec)
    ‚Üì
CNN (d√©tection de patterns)
    ‚Üì
LSTM (compr√©hension du contexte)
    ‚Üì
Classification (Positif/Neutre/N√©gatif)
```

## üìÅ Structure du Projet

```
commentaires-film/
‚îú‚îÄ‚îÄ data/                      # Donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ text_sentiment.csv    # Dataset des commentaires
‚îú‚îÄ‚îÄ models/                    # Mod√®les sauvegard√©s
‚îÇ   ‚îú‚îÄ‚îÄ w2v.wv                # Mod√®le Word2Vec
‚îÇ   ‚îî‚îÄ‚îÄ comment_sentiment_rnn.keras  # Mod√®le RNN
‚îú‚îÄ‚îÄ logs/                      # Logs TensorBoard
‚îú‚îÄ‚îÄ train_model.py            # Script d'entra√Ænement
‚îú‚îÄ‚îÄ predict.py                # Script de pr√©diction
‚îú‚îÄ‚îÄ test_gpu.py              # Test de configuration GPU
‚îú‚îÄ‚îÄ requirements.txt         # D√©pendances Python
‚îî‚îÄ‚îÄ .env                     # Configuration

```

## üöÄ Installation

### 1. Cloner le projet

```bash
git clone <votre-repo>
cd commentaires-film
```

### 2. Cr√©er un environnement virtuel (recommand√©)

```bash
# Linux/Mac
python3 -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### 4. T√©l√©charger le mod√®le de langue fran√ßaise

```bash
python -m spacy download fr_core_news_sm
```

### 5. Configuration (optionnel)

Modifier le fichier `.env` pour ajuster les param√®tres :

```env
W2V_SIZE=100        # Dimension des vecteurs Word2Vec
W2V_MIN_COUNT=3     # Fr√©quence minimale des mots
MAX_LENGTH=64       # Nombre max de mots par commentaire
NB_COMMENT=10000    # Nombre de commentaires pour l'entra√Ænement
```

## üíª Utilisation

### Entra√Æner le mod√®le

```bash
python train_model.py
```

Ce script va :
1. Charger les donn√©es depuis `data/text_sentiment.csv`
2. Entra√Æner un mod√®le Word2Vec pour cr√©er des embeddings
3. Entra√Æner un r√©seau CNN+LSTM pour la classification
4. Sauvegarder les mod√®les dans le dossier `models/`

**Temps d'entra√Ænement estim√©** : 
- CPU : ~30-45 minutes pour 10,000 commentaires
- GPU : ~5-10 minutes pour 10,000 commentaires

### Faire une pr√©diction

```bash
python predict.py "Ce film √©tait vraiment excellent, je le recommande !"
```

**Exemple de sortie** :
```
‚úÖ Mod√®le RNN charg√© depuis models/comment_sentiment_rnn.keras
‚úÖ Mod√®le Word2Vec charg√© depuis models/w2v.wv

üìä Analyse du commentaire:
Commentaire: "Ce film √©tait vraiment excellent, je le recommande !"
Sentiment pr√©dit: Positif (confiance: 92.3%)
Probabilit√©s d√©taill√©es:
  - N√©gatif: 5.2%
  - Neutre: 2.5%
  - Positif: 92.3%
```

### Tester la configuration GPU (optionnel)

```bash
python test_gpu.py
```

## üìä Dataset

Le fichier `data/text_sentiment.csv` doit contenir les colonnes suivantes :
- `comment` : Le texte du commentaire
- `negative` : Score de n√©gativit√© (0-1)
- `neutral` : Score de neutralit√© (0-1)
- `positive` : Score de positivit√© (0-1)

## üîß Personnalisation

### Modifier l'architecture du r√©seau

Dans `train_model.py`, fonction `create_rnn()` :

```python
def create_rnn():
    return tf.keras.Sequential([
        layers.Input(shape=(W2V_SIZE, MAX_LENGTH)),
        layers.Convolution1D(32, kernel_size=3, ...),  # Ajuster les filtres
        layers.LSTM(128, ...),  # Ajuster les neurones LSTM
        layers.Dense(64, ...),  # Ajuster la couche dense
        layers.Dense(3, activation="softmax")
    ])
```

### Choisir l'optimiseur

Dans `train_model.py`, lors de la compilation du mod√®le :

```python
rnn.compile(
    optimizer='adam',  # Optimiseur par d√©faut (recommand√©)
    loss="categorical_crossentropy",
    metrics=['categorical_accuracy']
)
```

**Optimiseurs disponibles et cas d'usage :**

| Optimiseur | Utilisation | Avantages | Cas d'usage id√©al |
|------------|-------------|-----------|-------------------|
| **adam** (d√©faut) | 90% des cas | Adaptatif, converge rapidement, robuste | NLP, vision, r√©seaux profonds, analyse de sentiments |
| **sgd** | Mod√®les simples | Simple, g√©n√©ralise bien | Fine-tuning, convergence finale pr√©cise |
| **rmsprop** | RNN, LSTM | Bon pour gradients instables | S√©ries temporelles, probl√®mes avec gradients variables |
| **adagrad** | Donn√©es √©parses | Adapte le learning rate par param√®tre | Embeddings de mots, texte avec vocabulaire large |
| **adamax** | Gradients bruit√©s | Plus stable qu'Adam | Mod√®les avec beaucoup de bruit |
| **nadam** | Convergence rapide | Adam + momentum Nesterov | Alternative √† Adam pour convergence plus rapide |
| **adadelta** | Sans hyperparam√®tres | Pas besoin de learning rate | Prototypage rapide |

**Configuration avanc√©e (optionnel) :**

```python
from keras.optimizers import Adam

# Personnaliser l'optimiseur
optimizer = Adam(
    learning_rate=0.001,  # Taux d'apprentissage
    beta_1=0.9,          # Momentum exponentiel pour le gradient
    beta_2=0.999         # Momentum exponentiel pour le carr√© du gradient
)

rnn.compile(
    optimizer=optimizer,
    loss="categorical_crossentropy",
    metrics=['categorical_accuracy']
)
```

**Recommandations pour votre cas (analyse de sentiments) :**
- ‚úÖ **Adam** : Excellent choix par d√©faut, fonctionne tr√®s bien pour l'analyse de sentiments
- **Alternative 1** : **Nadam** si la convergence est lente
- **Alternative 2** : **RMSprop** si vous observez des instabilit√©s pendant l'entra√Ænement
- **Alternative 3** : **SGD avec momentum** pour un fine-tuning final apr√®s Adam

üí° **Conseil** : Commencez toujours avec Adam. Changez d'optimiseur uniquement si vous rencontrez des probl√®mes sp√©cifiques de convergence ou de performance.

### Choisir la fonction de perte

Dans `train_model.py`, lors de la compilation du mod√®le :

```python
rnn.compile(
    optimizer='adam',
    loss="categorical_crossentropy",  # Fonction de perte par d√©faut pour multi-classes
    metrics=['categorical_accuracy']
)
```

**Fonctions de perte disponibles et cas d'usage :**

#### Pour la classification

| Fonction de perte | Cas d'usage | Activation finale | Exemple d'utilisation |
|------------------|-------------|-------------------|----------------------|
| **categorical_crossentropy** (d√©faut) | Classification multi-classes avec one-hot encoding | softmax | Sentiments (positif/neutre/n√©gatif), cat√©gories de films |
| **sparse_categorical_crossentropy** | Classification multi-classes avec labels entiers | softmax | M√™me cas mais labels [0,1,2] au lieu de [[1,0,0],[0,1,0],[0,0,1]] |
| **binary_crossentropy** | Classification binaire ou multi-label | sigmoid | Bon/mauvais film, tags multiples (action ET com√©die) |
| **focal_crossentropy** | Classification avec classes d√©s√©quilibr√©es | softmax/sigmoid | Dataset avec 90% positifs, 10% n√©gatifs |

#### Pour la r√©gression (si vous pr√©disez des scores)

| Fonction de perte | Cas d'usage | Activation finale | Exemple d'utilisation |
|------------------|-------------|-------------------|----------------------|
| **mean_squared_error** (MSE) | Pr√©diction de valeurs continues | linear/None | Note de 0 √† 10, score de sentiment 0-100% |
| **mean_absolute_error** (MAE) | R√©gression robuste aux outliers | linear/None | Scores avec donn√©es bruit√©es |
| **huber** | Hybride MSE/MAE | linear/None | Robuste mais diff√©rentiable |
| **mean_squared_logarithmic_error** | Valeurs avec large √©chelle | linear/None | Pr√©dictions o√π l'erreur relative compte plus |

#### Pour des cas sp√©cialis√©s

| Fonction de perte | Cas d'usage | Activation finale | Exemple d'utilisation |
|------------------|-------------|-------------------|----------------------|
| **cosine_similarity** | Similarit√© entre vecteurs | None | Comparaison d'embeddings de commentaires |
| **kullback_leibler_divergence** | Divergence entre distributions | softmax | Mod√®les g√©n√©ratifs, autoencoders |
| **poisson** | Comptage d'√©v√©nements | exponential | Nombre de likes, vues |

**Configuration pour votre cas (analyse de sentiments 3 classes) :**

```python
# Option 1 : One-hot encoding (votre configuration actuelle) ‚úÖ
# Labels : [[1,0,0], [0,1,0], [0,0,1]]
rnn.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # Parfait pour votre cas
    metrics=['categorical_accuracy']
)

# Option 2 : Labels entiers (plus √©conome en m√©moire)
# Labels : [0, 1, 2]
rnn.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)

# Option 3 : Si vous aviez des classes d√©s√©quilibr√©es
from tensorflow.keras.losses import CategoricalFocalCrossentropy
rnn.compile(
    optimizer='adam',
    loss=CategoricalFocalCrossentropy(alpha=0.25, gamma=2.0),
    metrics=['categorical_accuracy']
)
```

**Fonction de perte personnalis√©e (avanc√©) :**

```python
import tensorflow as tf

def custom_weighted_loss(y_true, y_pred):
    """Perte personnalis√©e avec poids diff√©rents par classe"""
    # Poids : n√©gatif=2, neutre=1, positif=1.5
    weights = tf.constant([2.0, 1.0, 1.5])
    
    # Categorical crossentropy pond√©r√©e
    cce = tf.keras.losses.CategoricalCrossentropy()
    base_loss = cce(y_true, y_pred)
    
    # Appliquer les poids selon la vraie classe
    class_weights = tf.reduce_sum(y_true * weights, axis=-1)
    weighted_loss = base_loss * class_weights
    
    return tf.reduce_mean(weighted_loss)

rnn.compile(
    optimizer='adam',
    loss=custom_weighted_loss,
    metrics=['categorical_accuracy']
)
```

**Recommandations pour votre projet :**
- ‚úÖ **categorical_crossentropy** : Excellent choix pour 3 classes √©quilibr√©es
- **Alternative 1** : **sparse_categorical_crossentropy** si vous voulez √©conomiser de la m√©moire
- **Alternative 2** : **focal_crossentropy** si vos classes sont tr√®s d√©s√©quilibr√©es
- **Alternative 3** : Fonction personnalis√©e si certains sentiments sont plus importants √† d√©tecter

üí° **Conseil** : La fonction de perte doit correspondre √† votre probl√®me ET √† votre activation finale. Pour la classification multi-classes, utilisez toujours softmax + categorical_crossentropy (ou sa variante sparse).

### Ajuster les hyperparam√®tres d'entra√Ænement

Dans `train_model.py`, fonction `main()` :

```python
rnn.fit(
    epochs=30,      # Nombre d'√©poques
    batch_size=32,  # Taille des batchs
    ...
)
```

## üìà Monitoring avec TensorBoard

Pour visualiser l'entra√Ænement en temps r√©el :

```bash
tensorboard --logdir=logs/rnn_movies_comments
```

Puis ouvrir : http://localhost:6006

## üêõ R√©solution de probl√®mes

### Erreur CUDA/GPU

Si TensorFlow ne d√©tecte pas votre GPU :

1. V√©rifier les drivers NVIDIA :
```bash
nvidia-smi
```

2. Installer TensorFlow avec support CUDA :
```bash
pip install tensorflow[and-cuda]
```

### Erreur de m√©moire

Si vous manquez de m√©moire, r√©duire :
- `NB_COMMENT` dans `.env`
- `batch_size` dans `train_model.py`

### Mot hors vocabulaire

Si un mot n'est pas reconnu lors de la pr√©diction, il est ignor√© (remplac√© par des z√©ros). Pour am√©liorer la couverture :
- Augmenter `NB_COMMENT` pour l'entra√Ænement
- Diminuer `W2V_MIN_COUNT` dans `.env`

## üìù D√©pendances principales

- `tensorflow[and-cuda]` : Framework de deep learning
- `gensim` : Pour Word2Vec
- `spacy` : Traitement du texte fran√ßais
- `pandas` : Manipulation des donn√©es
- `numpy` : Calculs num√©riques
- `scikit-learn` : Division train/test
- `python-dotenv` : Gestion de la configuration

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :
- Signaler des bugs
- Proposer de nouvelles fonctionnalit√©s
- Am√©liorer la documentation

## üìÑ Licence

MIT

## üë§ Auteur

Rapha√´l L√©o

---

**Note** : Ce projet est √† but √©ducatif pour apprendre le deep learning et le NLP.